{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use nothing more than Bayes' Theorem for a classification model if we make a few simplifications first.  Bayes' Theorem assumes that all input variables are dependent on all other variables.  The fundamental assumption in the Naive Bayes model is that the features are **independent** of each other.  By making this assumption, we are able to factor the probabilities in the equation from dependent conditional probabilities to independent conditional probabilities.  \n",
    "\n",
    "We can first get rid of the denominator in the equation $P(x1, x2, ..., xn)$ since it is not dependent on $y$.  So that leaves us with: \n",
    "\n",
    "$P(y_{i}|x_{1},...,x_{n}) = P(x_{1},...,x_{n}|y_{i}) * P(y_{i})$\n",
    "\n",
    "Then, the conditional probability of all variables given the class label is changed into separate conditional probabilities of each variable value given the class label.  These are then multiplied together: \n",
    "\n",
    "$P(y_{i}|x_{1},...,x_{n}) = P(x_{1}|y_{1}) *... * P(x_{n}|y_{1})$\n",
    "\n",
    "We can do this for each of the class labels and choose the label with the largest probability to be the classification.  This is the maximum a posteriori (MAP) decision rule.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see an example, let's create 100 instances with two numerical features, each assigned to one of two classes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n",
      "[[-2.98837186  8.82862715]\n",
      " [ 5.72293008  3.02697174]\n",
      " [-3.05358035  9.12520872]\n",
      " [ 5.461939    3.86996267]\n",
      " [ 4.86733877  3.28031244]]\n",
      "[0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Create small dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=42)\n",
    "print(X.shape, y.shape)\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can model the numerical input variables using a Gaussian distribution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a probability distribution to a univariate data sample\n",
    "from scipy.stats import norm\n",
    "def fit_distribution(data):\n",
    "    \n",
    "    # Estimate parameters\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.std(data)\n",
    "    print(mu, sigma)\n",
    "    \n",
    "    # Fit distribution \n",
    "    dist = norm(mu, sigma)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the conditional probability of each input variable.  So this means we need one distribution for each of the input variables, and one set of distributions for the class labels, with four in total: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2) (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Sort data into classes \n",
    "Xy0 = X[y == 0]\n",
    "Xy1 = X[y == 1]\n",
    "print(Xy0.shape, Xy1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these two groups, we can use them to calculate the priors for a sample belonging to either group.  We know this is going to be 50% since we created this dataset, but in a real situation, these would typically be different: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate priors \n",
    "prior0 = len(Xy0) / len(X)\n",
    "prior1 = len(Xy1) / len(X)\n",
    "print(prior0, prior1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we call the `fit_distribution()` function to generate a probability distribution for each variable and each class label: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.702923013045944 0.832080242600683\n",
      "8.89011496146507 0.9529082761802956\n",
      "4.608404434892749 0.8720134236486649\n",
      "2.1699819200257497 0.9986664144394607\n"
     ]
    }
   ],
   "source": [
    "# Create PDFs for y == 0\n",
    "X1_y0 = fit_distribution(Xy0[:, 0])\n",
    "X2_y0 = fit_distribution(Xy0[:, 1])\n",
    "\n",
    "# Create PDFs for y == 1\n",
    "X1_y1 = fit_distribution(Xy1[:, 0])\n",
    "X2_y1 = fit_distribution(Xy1[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the mean and standard deviation of each distribution printed, demonstrating that they are different.  We now use the prepared probabilistic model to make a prediction.  The independent conditional probability distribution for each class label can be calculated using the prior for the class (50%) and the conditional probability of the value for each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate independent conditional probability\n",
    "def probability(X, prior, dist1, dist2):\n",
    "    return prior * dist1.pdf(X[0]) * dist2.pdf(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to calculate the probability for an example belonging to each class.  Let's classify one example:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=0 | [-2.98837186  8.82862715]) = 9.443\n",
      "P(y=1 | [-2.98837186  8.82862715]) = 0.000\n",
      "Truth: y=0\n"
     ]
    }
   ],
   "source": [
    "# Classify one example\n",
    "Xsample, ysample = X[0], y[0]\n",
    "py0 = probability(Xsample, prior0, X1_y0, X2_y0)\n",
    "py1 = probability(Xsample, prior1, X1_y1, X2_y1)\n",
    "print('P(y=0 | %s) = %.3f' % (Xsample, py0*100))\n",
    "print('P(y=1 | %s) = %.3f' % (Xsample, py1*100))\n",
    "print('Truth: y=%d' % ysample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, there are three implementations of the Naive Bayes model: `BernoulliNB`, `MultinomialNB`, and `GaussianNB`.  Let's look at an example of the Gaussian form: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities: [[1.00000000e+00 7.10993754e-27]]\n",
      "Predicted class: [0]\n",
      "Truth: 0\n"
     ]
    }
   ],
   "source": [
    "# Example of Gaussian NB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = GaussianNB()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Select a single sample\n",
    "Xsample, ysample = [X[0]], y[0]\n",
    "\n",
    "# Make a probabilistic prediction\n",
    "yhat_prob = model.predict_proba(Xsample)\n",
    "print(f'Predicted probabilities: {yhat_prob}')\n",
    "\n",
    "# Make classification prediction\n",
    "yhat_class = model.predict(Xsample)\n",
    "print(f'Predicted class: {yhat_class}')\n",
    "print(f'Truth: {ysample}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the probability of the example belonging to y=0 is 1.0, or a certainty.  The probability of y=1 is very small, essentially zero.  Finally, the class label is predicted.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tips When Using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the probability distribution for a variable is complex or uknown, it can be a good idea to use a kernel density estimator, or KDE.  This allows us to approximate the distribution from the data samples.\n",
    "\n",
    "- Naive Bayes assumes the variables are independent, even though this is rarely true in the real world.  Still, it makes good approximations often when that isn't the case.  However, the more dependent the variables are, the less well the model is able to perform. \n",
    "\n",
    "- When calculating the independent conditional probability for one example for one class label, we have to multiply a bunch of individual probabilities together.  This can be unstable if the probabilities are small.  A trick is to transform the multiplication into log additions.  This is the 'log trick'. \n",
    "\n",
    "- When new data becomes available, we can use the new data with the old data to update the estimates of the parameters for each variable's probability distribution \n",
    "\n",
    "- The probability distributions will summarize the conditional probability of each input variable value for each class label.  We can use these distributions to randomly sample and create new plausible data instances.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
